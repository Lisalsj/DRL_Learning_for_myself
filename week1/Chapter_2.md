# 马尔可夫决策过程
## 马尔可夫过程（MP）
**马尔可夫性质**：指一个随机过程在给定现在的状态以及过去的所有状态的情况下，其未来状态的概率分布仅依赖于当前状态。

$$
p(X_{t+1} = x_{t+1} \mid X_{0:t} = x_{0:t}) = p(X_{t+1} = x_{t+1} \mid X_t = x_t) \tag{2.1}
$$

前面那个概率公式包含了0到上一时间步t的所有，等价于只有上一时间步t


设历史状态为 $h_t = {s_1,s_2,...,s_t}$ ,马尔可夫过程满足下列条件

$$
p(s_{t+1} \mid s_t) = p(s_{t+1} \mid h_t)
$$

**马尔可夫链**：有点像有向图，节点表示状态s_N，边表示转移概率。可以用**状态转移矩阵**$P$来表示

## 马尔可夫奖励过程（MRP）
大部分与马尔可夫链一样，但是多了奖励函数，R是一个期望，表示当达到某一个状态的时候可以获得多大的奖励，增加一个折扣因子来对预估未来的回报进行，折扣因子的作用是距离当前状态越远（未来的某一个状态）的预估奖励对当前状态的影响越小（避免奖励无穷无尽）

**折扣回报**：
$$
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \gamma^3 r_{t+4} + \dots + \gamma^{T-t-1} r_T
$$

**状态价值函数**：字面意思，该状态的价值
$$
V^t(s) = \mathbb{E} \left[ G_t \mid s_t = s \right] \\
       = \mathbb{E} \left[ r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots + \gamma^{T-t-1} r_T \mid s_t = s \right]
$$

## 贝尔曼方程
贝尔曼方程定义了状态之间的迭代关系
从价值函数冲推导出贝尔曼方程：
$$
V(s) = \underbrace{R(s)}_{\text{即时奖励}} + \gamma \underbrace{\sum_{s' \in S} p(s' \mid s) V(s')}_{\text{未来奖励的折扣总和}}
$$

当前状态的奖励 = 即时奖励 + 未来奖励的折扣总和。

未来奖励的的折扣总和：是对转移到未来某一个状态s'的概率与s'的状态价值的乘积的求和(未来所有可能发生的状态的和)

### 全期望公式与贝尔曼方程的推导
（书的第25 页，公式推导，感觉过一遍就行，学过概率论和线代的应该都看得懂）





